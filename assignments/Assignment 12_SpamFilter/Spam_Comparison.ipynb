{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply TF-IDF and develop Spam Filter Model for the below Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = '''I wanted to let you know about money that is\n",
    "available for college in your State .\n",
    "The amount is up to $5,730* if you qualify.\n",
    "It takes like 2 minutes to check if you\n",
    "qualify.\n",
    "Click Here to get matched.'''\n",
    "doc2 = '''Free -Coupons for next movie. T he above links will take you\n",
    "straight to our partner's site. For more information or to\n",
    "see other offers available, you can also visit the Groupon\n",
    "on the Working Advantage website.'''\n",
    "doc3 = '''Our records indicate your Pension is under performing to see higher\n",
    "growth and up to 25% cash release reply PENSION for a free review.\n",
    "To opt out reply STOP'''\n",
    "doc4 = '''Enter to win $25,000 and get a Free Hotel Night! Just\n",
    "click here for a $1 trial membership in NetMarket, the\n",
    "Internet'spremier discount shopping site:\n",
    "Fast Company EZVenture gives you FREE business\n",
    "articles,PLUS, you could win YOUR CHOICE of a BMW Z3\n",
    "convertible, $100,000, shares of Microsoft stock, or a\n",
    "home office computer. Go there and get your chances to\n",
    "win now.\n",
    "A crazy-funny-cool trivia book with a $10,000 prize?\n",
    "PLUS chocolate, nail polish, cats, barnyard animals,\n",
    "and more?'''\n",
    "doc5 = '''Dear recipient,\n",
    "Avangar Technologies announces the beginning of a new\n",
    "unprecendented global employment campaign.\n",
    "Due to company's exploding growth Avangar is expanding\n",
    "business to the European region.\n",
    "During last employment campaign over 1500 people worldwide\n",
    "took part in Avangar's business\n",
    "and more than half of them are currently employed by the\n",
    "company. And now we are offering you\n",
    "one more opportunity to earn extra money working with Avangar\n",
    "Technologies.\n",
    "We are looking for honest, responsible, hard-working people that\n",
    "can dedicate 2-4 hours of their\n",
    "time per day and earn extra Â£300-500 weekly. All offered\n",
    "positions are currently part-time\n",
    "and give you a chance to work mainly from home.'''\n",
    "doc6 = '''I know that's an incredible statement, but bear with me\n",
    "while I explain. You have already deleted mail from\n",
    "dozens of \"Get Rich Quick\" schemes, chain letter\n",
    "offers, and LOTS of other absurd scams that promise to\n",
    "make you rich overnight with no investment and no work.\n",
    "My offer isn't one of those. What I'm offering is a\n",
    "straightforward computer-based service that you can run\n",
    "full-or part-time like a regular business. This service\n",
    "runs auto-matically while you sleep, vacation, or work\n",
    "a \"regular\" job. It provides a valuable new service for\n",
    "businesses in your area.\n",
    "I'm offering a high-tech, low-maintenance, work-fromanywhere\n",
    "business that can bring in a nice comfortable\n",
    "additional income for your family. I did it for eight\n",
    "years. Since I started inviting others to join me, I've\n",
    "helped over 4000 do the same.'''\n",
    "\n",
    "doc_spam = 'Free\tClick here\tvisit\topen attachment\tcall this number\tmoney\tOut\textra\toffer\tavailable\tPension\tOpportunity\tChance\tInvestment'\n",
    "# combine documents\n",
    "#corpus_spam = ['Free',\t'Click here',\t'visit',\t'open attachment',\t'call this number',\t'money',\t'Out',\t'extra',\t'offer',\t'available',\t'Pension',\t'Opportunity',\t'Chance',\t'Investment']\n",
    "#corpus_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextCleansing(txt):\n",
    "    rem = re.sub('[^a-zA-Z]', ' ', txt)\n",
    "    rem = rem.lower()\n",
    "    rem = rem.split()\n",
    "    ps = PorterStemmer()\n",
    "    rem = [ps.stem(word) for word in rem if not word in set(stopwords.words('english'))]\n",
    "    rem = ' '.join(rem)\n",
    "    return rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['want let know money avail colleg state amount qualifi take like minut check qualifi click get match',\n",
       " 'free coupon next movi link take straight partner site inform see offer avail also visit groupon work advantag websit',\n",
       " 'record indic pension perform see higher growth cash releas repli pension free review opt repli stop',\n",
       " 'enter win get free hotel night click trial membership netmarket internet spremier discount shop site fast compani ezventur give free busi articl plu could win choic bmw z convert share microsoft stock home offic comput go get chanc win crazi funni cool trivia book prize plu chocol nail polish cat barnyard anim',\n",
       " 'dear recipi avangar technolog announc begin new unprecend global employ campaign due compani explod growth avangar expand busi european region last employ campaign peopl worldwid took part avangar busi half current employ compani offer one opportun earn extra money work avangar technolog look honest respons hard work peopl dedic hour time per day earn extra weekli offer posit current part time give chanc work mainli home',\n",
       " 'know incred statement bear explain alreadi delet mail dozen get rich quick scheme chain letter offer lot absurd scam promis make rich overnight invest work offer one offer straightforward comput base servic run full part time like regular busi servic run auto matic sleep vacat work regular job provid valuabl new servic busi area offer high tech low mainten work fromanywher busi bring nice comfort addit incom famili eight year sinc start invit other join help',\n",
       " 'free click visit open attach call number money extra offer avail pension opportun chanc invest']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Puja = []\n",
    "Puja.append(TextCleansing(doc1))\n",
    "Puja.append(TextCleansing(doc2))\n",
    "Puja.append(TextCleansing(doc3))\n",
    "Puja.append(TextCleansing(doc4))\n",
    "Puja.append(TextCleansing(doc5))\n",
    "Puja.append(TextCleansing(doc6))\n",
    "Puja.append(TextCleansing(doc_spam))\n",
    "Puja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x183 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 222 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_data = vectorizer.fit_transform(Puja)\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.31283772, 0.        , 0.22196771, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31283772, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.22196771, 0.        , 0.        ,\n",
       "        0.        , 0.22196771, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.25968217, 0.        , 0.        ,\n",
       "        0.        , 0.1927141 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25968217, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.22196771, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.31283772, 0.1927141 , 0.        , 0.        , 0.31283772,\n",
       "        0.25968217, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25968217, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25968217, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_\n",
    "X_data.shape\n",
    "X_data[6].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine distance to check the degree of Spam document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8821777966075723"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cos_doc1 = spatial.distance.cosine(X_data[0].toarray(), X_data[6].toarray())\n",
    "cos_doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8450471992863455"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_doc2 = spatial.distance.cosine(X_data[1].toarray(), X_data[6].toarray())\n",
    "cos_doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8686164114217365"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_doc3 = spatial.distance.cosine(X_data[2].toarray(), X_data[6].toarray())\n",
    "cos_doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9258398584864185"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_doc4 = spatial.distance.cosine(X_data[3].toarray(), X_data[6].toarray())\n",
    "cos_doc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8771233220819187"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_doc5 = spatial.distance.cosine(X_data[4].toarray(), X_data[6].toarray())\n",
    "cos_doc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9272835005284823"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_doc6 = spatial.distance.cosine(X_data[5].toarray(), X_data[6].toarray())\n",
    "cos_doc6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "From above result, we can conclude that Document 1 , Document 4 and Document 6 contain high percentage of spam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
